{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import concurrent.futures\n",
    "import inspect\n",
    "import pickle\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from Datacheck import encoder, evensampler, sampler, splittuple\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import (GridSearchCV, RepeatedKFold,\n",
    "                                     StratifiedKFold)\n",
    "\n",
    "\n",
    "def splitter(list,n):\n",
    "    split = int(len(list)/n) \n",
    "    for i in range(0, len(list),split):\n",
    "        print(i+split)\n",
    "        yield list[i:i + split]\n",
    "def permuter(list):\n",
    "    combinations = []\n",
    "    for i in range(len(list[0])):\n",
    "        combinations.extend(grapher([],[list[0][i]],list[1:]))\n",
    "    return combinations\n",
    "def grapher(permutations,stable,rest):\n",
    "    if len(rest) == 0:\n",
    "        permutations.append(stable)\n",
    "        return stable\n",
    "    else:\n",
    "        for i in range(len(rest[0])):\n",
    "            #rest[0][i] in array for it to be able to concat to append without \n",
    "            #changing the original reference\n",
    "            temp = stable+[rest[0][i]]\n",
    "            #slicing the array past its end doesnt give an out of bound\n",
    "            #it gives an empty array\n",
    "            grapher(permutations,temp,rest[1:])\n",
    "    return permutations\n",
    "\n",
    "def tuning(X,Y,model,shape,vals,params = [],batch = 32,epoch = 50,cv = 5,repeat = 1):\n",
    "    #gettign the parameters and base values of the function\n",
    "    base = inspect.getfullargspec(eval(model))\n",
    "    #getting the base values for the eval for it to be the local dict for eval\n",
    "    basedict = dict(zip(base[0][1:],base[3])) \n",
    "    summary = []\n",
    "    best = 0\n",
    "    bestparam = []\n",
    "    bestmodel = 0\n",
    "    #the batches and params are put here because the comaprison of sets is put down there if it was only to batches and epochs\n",
    "    #i'd save the processing done with the sets and use -3 index of the keys index but after one iteration it for some reason adds\n",
    "    #a builtin element to the dictionary when used as a local identifier for the eval method\n",
    "    if \"batches\" not in params:\n",
    "        basedict[\"batches\"] = batch\n",
    "    if \"epochs\" not in params:\n",
    "        basedict[\"epochs\"] = epoch\n",
    "    #this is put here as well to save the processing of identifying it several times inside the loop\n",
    "    basedict[model]= eval(model)\n",
    "    #the current fast optimization for the code if i remove the set compariosn and just use the -3 indexing to remove\n",
    "    #batches,epochs,model from teh input str the thing is i dont know when and why does the builtin get added to the dict\n",
    "    for elem in vals:\n",
    "        #current values of each updated parameter\n",
    "        updater = dict(zip(params,elem))\n",
    "        basedict.update(updater)\n",
    "        keys = list(basedict.keys())\n",
    "        diff = 1 if '__builtins__' in keys else 0 \n",
    "        inputstr = \",\".join(f\"{key}\" for key in keys[:(-3-diff)])\n",
    "        print(inputstr)\n",
    "        kfolds = StratifiedKFold(n_splits = cv,random_state = random.randint(0,60000))\n",
    "        #gets the avg of the kfolds\n",
    "        avggloballocal = 0\n",
    "        #tries to get the bestinstance of all kfolds which doesnt really matter but helps you save the mdel\n",
    "        #although the important thign are the parameters\n",
    "        bestinstance = 0\n",
    "        #used to compare accuracy of instances needs to be reinitialized everytime\n",
    "        instance =0\n",
    "\n",
    "        print(kfolds.get_n_splits(X))\n",
    "        for train,test in kfolds.split(X,Y):\n",
    "            X_train ,X_test = X[train],X[test]\n",
    "            train_target,test_target = Y[train],Y[test]\n",
    "            mod = eval(f\"{model}({shape},{inputstr})\",basedict)\n",
    "\n",
    "            history,evaluation = fittertester(mod,X_train,train_target\n",
    "            ,X_test,test_target,batches = basedict[\"batches\"],epoch = basedict[\"epochs\"],metric = \"accuracy\")\n",
    "            #instance is 0 for every model to find max instance in model\n",
    "            if evaluation > instance:\n",
    "                bestinstance = mod\n",
    "            #avggloballocal accuracy which will be avgd in the end\n",
    "            avggloballocal += evaluation\n",
    "        avggloballocal /= cv\n",
    "        if avggloballocal >= best:\n",
    "            best = avggloballocal\n",
    "            bestparam = elem\n",
    "            bestmodel = bestinstance\n",
    "        string = \", \".join(f\"{param}:{basedict[param]}\" for param in params)\n",
    "        print(string)\n",
    "        summary.append(f\"accuracy :{evaluation} params:{string}\")\n",
    "    summary.append(\"BEST MODEL:\"+\",\".join(f\"{param}:{val}\" for param,val in zip(params,bestparam))+f\",accuracy:{best}\")\n",
    "    pickle.dump(bestmodel,open('GraduationProject/Emognition/bestmodel.pkl', 'wb'))\n",
    "    return summary\n",
    "\n",
    "\n",
    "def cnn1d(shape,state = \"compiled\",convlayers=1,layers = 2,unit = 100,func=\"softsign\",output=11,filter = 64,kernel = 21,initializer = \"he_uniform\",\n",
    "dropout= 0.5,droplayers =1,lossfun = \"sparse_categorical_crossentropy\",optimizers = \"Adamax\",metric = [\"accuracy\"]):\n",
    "    model = tf.keras.Sequential()\n",
    "    #kernel_initializer = \"glorot_uniform\" by default\n",
    "    model.add(tf.keras.layers.Conv1D(filters = filter,kernel_initializer=initializer,kernel_size=kernel,activation=func,input_shape = shape))\n",
    "    for i in range(convlayers):\n",
    "        if i < convlayers-1:\n",
    "            model.add(tf.keras.layers.Conv1D(filters=filter, kernel_size=kernel, activation=func))\n",
    "        for k in range(droplayers):\n",
    "            model.add(tf.keras.layers.Dropout(dropout))\n",
    "        model.add(tf.keras.layers.MaxPooling1D(pool_size=5))\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    if state == \"compiled\":\n",
    "        for _ in range(layers):\n",
    "            model.add(tf.keras.layers.Dense(units = unit,activation=func))\n",
    "        model.add(tf.keras.layers.Dense(output,activation=\"softmax\"))\n",
    "        model.compile(loss=lossfun, optimizer=optimizers, metrics= metric)\n",
    "    return model\n",
    "\n",
    "def lstmo(shape,state= \"compiled\",lstmlayer = 4,layers = 1,unitnormal = 100,func=\"relu\",output=11\n",
    ",unitlstm = 50,dropout= 0.5,lossfun = \"sparse_categorical_crossentropy\",optimizers = \"adam\",metric = [\"accuracy\"]):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.LSTM(units=unitlstm,return_sequences=True,input_shape=shape))\n",
    "    model.add(tf.keras.layers.Dropout(dropout))\n",
    "    for _ in range(lstmlayer-1):\n",
    "        model.add(tf.keras.layers.LSTM(units = unitlstm,return_sequences=True))\n",
    "        model.add(tf.keras.layers.Dropout(dropout))\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    if state == \"compiled\":\n",
    "        for _ in range(layers-1):\n",
    "            model.add(tf.keras.layers.Dense(units = unitnormal,activation=func))\n",
    "        model.add(tf.keras.layers.Dense(units = output,activation = \"softmax\"))\n",
    "        model.compile(loss=lossfun, optimizer=optimizers, metrics= metric)\n",
    "    return model\n",
    "\n",
    "#global identifier for metric measures to not be instantiated everytime function called\n",
    "verbo = {\"accuracy\":1,\"loss\":0,\"precision\":2,\"recall\":3}\n",
    "def fittertester(mod,train,train_target,test,test_target,batches = 32,epoch = 50,metric = \"accuracy\"):\n",
    "    \n",
    "    history = mod.fit(train,train_target,validation_split = 0.2\n",
    "        ,batch_size = batches,epochs = epoch,verbose = 1\n",
    "        ,callbacks = [\n",
    "            tf.keras.callbacks.EarlyStopping(\n",
    "                monitor = \"val_loss\",\n",
    "                patience = 5,\n",
    "                restore_best_weights=True\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    evaluation = mod.evaluate(test,test_target,verbose = 0)[verbo[metric]]\n",
    "    return history,evaluation\n",
    "\n",
    "#tihs works only on 1d cnn not even 2d neds to be optimized but idk how to rn\n",
    "def layervisualizer(model,layer,sample,path= \"./\",color = \"Spectral\"):\n",
    "    indices = []\n",
    "    for i,layers in enumerate(model.layers):\n",
    "        if layer in layers.name:\n",
    "            #get the index of the desired layer needed to visualize\n",
    "            indices.append(i)\n",
    "    #get_weights returns filters,biases so i get 0 because i want the filters weights only\n",
    "    filters = np.array([model.layers[ind].get_weights()[0] for ind in indices])\n",
    "    print(filters.shape[3])\n",
    "    for i in range(len(filters)):\n",
    "        for j in range(filters.shape[3]):\n",
    "            #this just means taking all the input values into consideration for each jth filter and ith layer\n",
    "            f = filters[i,:,:,j]\n",
    "            fig1 = plt.subplot(8,8,j+1)\n",
    "            fig1.set_xticks([])\n",
    "            fig1.set_yticks([])\n",
    "            plt.imshow(f,cmap = color,aspect = \"auto\")\n",
    "        plt.savefig(f\"{path}viz_{layer}_layer_{i+1}.png\",dpi = 1000)\n",
    "        plt.close()\n",
    "    \n",
    "    output = [model.layers[ind].output for ind in indices]\n",
    "    vizmodel = tf.keras.Model(inputs = model.inputs,outputs = output)\n",
    "    outp = vizmodel.predict(sample)\n",
    "    print(outp.shape)\n",
    "    #the for i in range is because the shape is (1,6270,64) and if i iterate\n",
    "    #over the outp itll only iterte for one time and wont iterate over each of the filters\n",
    "    for j in range(len(outp)):\n",
    "        for i in range(outp.shape[2]):\n",
    "            f = outp[:,:,i]\n",
    "            #subplot starts from 1\n",
    "            fig1 = plt.subplot(8,8,i+1)\n",
    "            fig1.set_xticks([])\n",
    "            fig1.set_yticks([])\n",
    "            plt.imshow(f,cmap = color,aspect = \"auto\")\n",
    "    plt.savefig(f\"{path}baseline_thru_{layer}_layer_{j+1}.png\",dpi = 1000)\n",
    "    plt.close()\n",
    "\n",
    "#abstracted into its own function because it only needs to be done once\n",
    "#and the multiheaded model could be initialized in the kfolds as its own module\n",
    "#if it doesnt work then probably each of the head models should be reinitialized everytime the multiheaded model is called\n",
    "#and if that happens the function built should be modified\n",
    "def flatter(models):\n",
    "    index = []\n",
    "    for model in models:\n",
    "        for i,layers in enumerate(model.layers):\n",
    "            if \"flatten\" in layers.name:\n",
    "                #get the index of the desired layer needed to visualize\n",
    "                index.append(i)\n",
    "    return index\n",
    "#im sure even if its not a problem it initializes the same random weights for each head\n",
    "#in each run so i dont know how much of a problem is it but it is only random on the heads\n",
    "#to remove that we have to reinitialize each model in each kfold as mentioned in the above section\n",
    "def headjoiner(models: list,index,layers = 1,unit = 100,func = \"softsign\",output = 11,lossfun = \"sparse_categorical_crossentropy\",optimizers = \"Adamax\",metric = [\"accuracy\"]):\n",
    "    #so why is it called by outputs makes it work for some reason i still dont know\n",
    "    #but it has something to do with teh sequential and functional api\n",
    "    #didnt work in this code\n",
    "    \"\"\"\n",
    "    produced by using this following code\n",
    "\n",
    "    def headjoiner(models: list,index,layers = 1,unit = 100,func = \"softsign\",output = 11,lossfun = \"sparse_categorical_crossentropy\",optimizers = \"Adamax\",metric = [\"accuracy\"]):\n",
    "    flats = [mod.layers[ind].output for ind,mod in zip(index,models)]\n",
    "    print(models[0].layers[index[0]]._inbound_nodes)\n",
    "    flats = np.array(flats)\n",
    "    ###########################################################\n",
    "    #this istn considered as aviable input needs to be checked\n",
    "    merged = tf.keras.layers.concatenate(inputs = flats,axis =-1)\n",
    "    #couldnt use it as add since merged isnt considered as a model you could add to \n",
    "    for _ in range(layers):\n",
    "        merged = tf.keras.layers.Dense(units = unit,activation=func)(merged)\n",
    "    merged = tf.keras.layers.Dense(output,activation=\"softmax\")(merged)\n",
    "    inputs = [model.inputs for model in models]\n",
    "    mergedmod = tf.keras.Model(input = inputs,output = merged)\n",
    "    mergedmod.compile(loss=lossfun, optimizer=optimizers, metrics= metric)\n",
    "    return mergedmod\n",
    "\n",
    "    TypeError: You are passing KerasTensor(type_spec=TensorSpec(shape=(None, 80256), \n",
    "    dtype=tf.float32, name=None), name='flatten_16/Reshape:0', description=\"created by layer 'flatten_16'\"),\n",
    "    an intermediate Keras symbolic input/output, to a TF API that does not allow registering custom dispatchers,\n",
    "    such as `tf.cond`, `tf.function`, gradient tapes, or `tf.map_fn`. Keras Functional model construction\n",
    "    only supports TF API calls that *do* support dispatching, such as `tf.math.add` or \n",
    "    `tf.reshape`. Other APIs cannot be called directly on symbolic Kerasinputs/outputs. \n",
    "    You can work around this limitation by putting the operation in a custom \n",
    "    Keras layer `call` and calling that layer on this symbolic input/output\n",
    "    \n",
    "    #might be helpful\n",
    "    https://stackoverflow.com/questions/44042173/concatenate-merge-layer-keras-with-tensorflow\n",
    "    \n",
    "    https://keras.io/guides/functional_api/\n",
    "\n",
    "    https://www.google.com/search?q=How+to+%22Merge%22+Sequential+models+in+tensorflow&ei=VkrfY97XGqvUkdUP0IiOkAY&ved=0ahUKEwje2rWg3v38AhUraqQEHVCEA2IQ4dUDCA8&uact=5&oq=How+to+%22Merge%22+Sequential+models+in+tensorflow&gs_lcp=Cgxnd3Mtd2l6LXNlcnAQAzIFCCEQoAE6CAgAEIYDELADSgQIQRgASgQIRhgAUPUEWLMGYJwIaABwAHgAgAGXAYgBhwOSAQMwLjOYAQCgAQHIAQXAAQE&sclient=gws-wiz-serp\n",
    "    \n",
    "    really useful link as well:\n",
    "    https://stackoverflow.com/questions/53942291/what-does-the-00-of-the-layers-connected-to-in-keras-model-summary-mean\n",
    "\n",
    "    explains how each layer is a node and carries the structure along wiht it no need to \n",
    "    \"\"\"\n",
    "    #note on .output\n",
    "    flats = [mod.layers[ind].output for ind,mod in zip(index,models)]\n",
    "    #print(models[0].layers[index[0]]._inbound_nodes)\n",
    "    ###########################################################\n",
    "    #this istn considered as aviable input needs to be checked\n",
    "    merged = tf.keras.layers.Concatenate()(flats)\n",
    "    #couldnt use it as add since merged isnt considered as a model you could add to \n",
    "    for _ in range(layers):\n",
    "        merged = tf.keras.layers.Dense(units = unit,activation=func)(merged)\n",
    "        merged = tf.keras.layers.Dropout(0.5)(merged)\n",
    "    merged = tf.keras.layers.Dense(output,activation=\"softmax\")(merged)\n",
    "    inputs = [model.inputs for model in models]\n",
    "    \n",
    "    mergedmod = tf.keras.Model(inputs,merged)\n",
    "    mergedmod.compile(loss=lossfun, optimizer=optimizers, metrics= metric)\n",
    "    return mergedmod\n",
    "\n",
    "def resultviz(path,model,hist,actual,test,uniques):\n",
    "    #predictions is the max probability over the predictions since each prediction element is an array of length 11 with probvabliities of each class\n",
    "    prediction = np.array(list(map(lambda x:np.argmax(x),model.predict(test))))\n",
    "    cs = confusion_matrix(actual,prediction)\n",
    "    clr = classification_report(actual,prediction,target_names = uniques,output_dict=True)\n",
    "    plt.figure(figsize = (10,10))\n",
    "    sns.heatmap(cs,annot = True,cmap = \"GnBu\")\n",
    "    plt.xticks(np.arange(len(uniques)),remapper)\n",
    "    plt.yticks(np.arange(len(uniques)),remapper)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.savefig(f\"{path}Heatmap.png\")\n",
    "    print(model.summary())\n",
    "    plt.close()\n",
    "    plt.plot(hist.history[\"loss\"],label=\"train loss\")\n",
    "    plt.plot(hist.history[\"val_loss\"],label=\"validation loss\")\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"{path}Trainingloss.png\")\n",
    "    plt.close()\n",
    "    plt.plot(hist.history['accuracy'],label='accuracy')\n",
    "    plt.savefig(f\"{path}accuracy.png\")\n",
    "    print(\"here\")\n",
    "    dataframe = pd.DataFrame(clr)\n",
    "    dataframe.to_csv(f\"{path}{model.layers[0].name}_report.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data = pd.read_pickle(\"E:/ABDO/Graduation project/Datasets/Emognition/hr(tuples).pkl\")\n",
    "\n",
    "    #it is turned into an object i dont know why but it's turned into an inta nd type changed before\n",
    "    #mask so i dont pass it through another func\n",
    "    remapper = data[\"Target\"].unique()\n",
    "    data[\"Target\"] = encoder(data,\"Target\")\n",
    "    #no need since new data is formed in hr only\n",
    "    data = splittuple(data,0)\n",
    "    databvp = pd.read_pickle(\"GraduationProject/Emognition/bvp_clean.pkl\")\n",
    "    data = data[data[\"Stage\"] == \"STIMULUS\"]\n",
    "    databvp = databvp[databvp[\"Stage\"] == \"STIMULUS\"]\n",
    "    #databvp = splittuple(databvp,1)\n",
    "    # get the target values\n",
    "    #put in an np.array as int idk why but it used to work without it now it \n",
    "    #reads its type not as an int but as an object while every var inside is an instance of an int\n",
    "\n",
    "#data splitting, and model prepping\n",
    "########################################################################################################################################################\n",
    "    datatarget = np.asarray(data[\"Target\"].values).astype(int)\n",
    "    #adding pp interval is dstacking two splits one split for the hr and one for the ppinterval\n",
    "    databvp = np.asarray(databvp.values[:,3:]).astype(float)\n",
    "    data =  np.asarray(data.values[:,2:-13]).astype(int)\n",
    "\n",
    "    model1 = cnn1d((data.shape[1],1))\n",
    "    model2 = cnn1d((databvp.shape[1],1))\n",
    "    #just for ease in writing\n",
    "    modlist = [model1,model2]\n",
    "    index = flatter(modlist)\n",
    "    \n",
    "#model training, and visualization\n",
    "###########################################################################################################################################################   \n",
    "    kfolds = StratifiedKFold(n_splits=4)\n",
    "    models = []\n",
    "    accuracy = []\n",
    "    histories = []\n",
    "    tests = [] \n",
    "    targets = []"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'GraduationProject/Emognition/bvp_clean.pkl'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32me:\\ABDO\\Courses\\Python\\GraduationProject\\Emognition\\1dcnn.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    <a href='file:///e%3A/ABDO/Courses/Python/GraduationProject/Emognition/1dcnn.py?line=303'>304</a>\u001b[0m     \u001b[1;31m#no need since new data is formed in hr only\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    <a href='file:///e%3A/ABDO/Courses/Python/GraduationProject/Emognition/1dcnn.py?line=304'>305</a>\u001b[0m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msplittuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> <a href='file:///e%3A/ABDO/Courses/Python/GraduationProject/Emognition/1dcnn.py?line=305'>306</a>\u001b[1;33m     \u001b[0mdatabvp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"GraduationProject/Emognition/bvp_clean.pkl\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    <a href='file:///e%3A/ABDO/Courses/Python/GraduationProject/Emognition/1dcnn.py?line=306'>307</a>\u001b[0m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Stage\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"STIMULUS\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    <a href='file:///e%3A/ABDO/Courses/Python/GraduationProject/Emognition/1dcnn.py?line=307'>308</a>\u001b[0m     \u001b[0mdatabvp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatabvp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdatabvp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Stage\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"STIMULUS\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\user\\.conda\\envs\\tf2.11\\lib\\site-packages\\pandas\\io\\pickle.py\u001b[0m in \u001b[0;36mread_pickle\u001b[1;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n\u001b[0;32m    188\u001b[0m     \"\"\"\n\u001b[0;32m    189\u001b[0m     \u001b[0mexcs_to_catch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mAttributeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModuleNotFoundError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 190\u001b[1;33m     with get_handle(\n\u001b[0m\u001b[0;32m    191\u001b[0m         \u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m         \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\user\\.conda\\envs\\tf2.11\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    863\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    864\u001b[0m             \u001b[1;31m# Binary mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 865\u001b[1;33m             \u001b[0mhandle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    866\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    867\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'GraduationProject/Emognition/bvp_clean.pkl'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import concurrent.futures\n",
    "import inspect\n",
    "import pickle\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from Datacheck import encoder, evensampler, sampler, splittuple\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import (GridSearchCV, RepeatedKFold,\n",
    "                                     StratifiedKFold)\n",
    "\n",
    "\n",
    "def splitter(list,n):\n",
    "    split = int(len(list)/n) \n",
    "    for i in range(0, len(list),split):\n",
    "        print(i+split)\n",
    "        yield list[i:i + split]\n",
    "def permuter(list):\n",
    "    combinations = []\n",
    "    for i in range(len(list[0])):\n",
    "        combinations.extend(grapher([],[list[0][i]],list[1:]))\n",
    "    return combinations\n",
    "def grapher(permutations,stable,rest):\n",
    "    if len(rest) == 0:\n",
    "        permutations.append(stable)\n",
    "        return stable\n",
    "    else:\n",
    "        for i in range(len(rest[0])):\n",
    "            #rest[0][i] in array for it to be able to concat to append without \n",
    "            #changing the original reference\n",
    "            temp = stable+[rest[0][i]]\n",
    "            #slicing the array past its end doesnt give an out of bound\n",
    "            #it gives an empty array\n",
    "            grapher(permutations,temp,rest[1:])\n",
    "    return permutations\n",
    "\n",
    "def tuning(X,Y,model,shape,vals,params = [],batch = 32,epoch = 50,cv = 5,repeat = 1):\n",
    "    #gettign the parameters and base values of the function\n",
    "    base = inspect.getfullargspec(eval(model))\n",
    "    #getting the base values for the eval for it to be the local dict for eval\n",
    "    basedict = dict(zip(base[0][1:],base[3])) \n",
    "    summary = []\n",
    "    best = 0\n",
    "    bestparam = []\n",
    "    bestmodel = 0\n",
    "    #the batches and params are put here because the comaprison of sets is put down there if it was only to batches and epochs\n",
    "    #i'd save the processing done with the sets and use -3 index of the keys index but after one iteration it for some reason adds\n",
    "    #a builtin element to the dictionary when used as a local identifier for the eval method\n",
    "    if \"batches\" not in params:\n",
    "        basedict[\"batches\"] = batch\n",
    "    if \"epochs\" not in params:\n",
    "        basedict[\"epochs\"] = epoch\n",
    "    #this is put here as well to save the processing of identifying it several times inside the loop\n",
    "    basedict[model]= eval(model)\n",
    "    #the current fast optimization for the code if i remove the set compariosn and just use the -3 indexing to remove\n",
    "    #batches,epochs,model from teh input str the thing is i dont know when and why does the builtin get added to the dict\n",
    "    for elem in vals:\n",
    "        #current values of each updated parameter\n",
    "        updater = dict(zip(params,elem))\n",
    "        basedict.update(updater)\n",
    "        keys = list(basedict.keys())\n",
    "        diff = 1 if '__builtins__' in keys else 0 \n",
    "        inputstr = \",\".join(f\"{key}\" for key in keys[:(-3-diff)])\n",
    "        print(inputstr)\n",
    "        kfolds = StratifiedKFold(n_splits = cv,random_state = random.randint(0,60000))\n",
    "        #gets the avg of the kfolds\n",
    "        avggloballocal = 0\n",
    "        #tries to get the bestinstance of all kfolds which doesnt really matter but helps you save the mdel\n",
    "        #although the important thign are the parameters\n",
    "        bestinstance = 0\n",
    "        #used to compare accuracy of instances needs to be reinitialized everytime\n",
    "        instance =0\n",
    "\n",
    "        print(kfolds.get_n_splits(X))\n",
    "        for train,test in kfolds.split(X,Y):\n",
    "            X_train ,X_test = X[train],X[test]\n",
    "            train_target,test_target = Y[train],Y[test]\n",
    "            mod = eval(f\"{model}({shape},{inputstr})\",basedict)\n",
    "\n",
    "            history,evaluation = fittertester(mod,X_train,train_target\n",
    "            ,X_test,test_target,batches = basedict[\"batches\"],epoch = basedict[\"epochs\"],metric = \"accuracy\")\n",
    "            #instance is 0 for every model to find max instance in model\n",
    "            if evaluation > instance:\n",
    "                bestinstance = mod\n",
    "            #avggloballocal accuracy which will be avgd in the end\n",
    "            avggloballocal += evaluation\n",
    "        avggloballocal /= cv\n",
    "        if avggloballocal >= best:\n",
    "            best = avggloballocal\n",
    "            bestparam = elem\n",
    "            bestmodel = bestinstance\n",
    "        string = \", \".join(f\"{param}:{basedict[param]}\" for param in params)\n",
    "        print(string)\n",
    "        summary.append(f\"accuracy :{evaluation} params:{string}\")\n",
    "    summary.append(\"BEST MODEL:\"+\",\".join(f\"{param}:{val}\" for param,val in zip(params,bestparam))+f\",accuracy:{best}\")\n",
    "    pickle.dump(bestmodel,open('GraduationProject/Emognition/bestmodel.pkl', 'wb'))\n",
    "    return summary\n",
    "\n",
    "\n",
    "def cnn1d(shape,state = \"compiled\",convlayers=1,layers = 2,unit = 100,func=\"softsign\",output=11,filter = 64,kernel = 21,initializer = \"he_uniform\",\n",
    "dropout= 0.5,droplayers =1,lossfun = \"sparse_categorical_crossentropy\",optimizers = \"Adamax\",metric = [\"accuracy\"]):\n",
    "    model = tf.keras.Sequential()\n",
    "    #kernel_initializer = \"glorot_uniform\" by default\n",
    "    model.add(tf.keras.layers.Conv1D(filters = filter,kernel_initializer=initializer,kernel_size=kernel,activation=func,input_shape = shape))\n",
    "    for i in range(convlayers):\n",
    "        if i < convlayers-1:\n",
    "            model.add(tf.keras.layers.Conv1D(filters=filter, kernel_size=kernel, activation=func))\n",
    "        for k in range(droplayers):\n",
    "            model.add(tf.keras.layers.Dropout(dropout))\n",
    "        model.add(tf.keras.layers.MaxPooling1D(pool_size=5))\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    if state == \"compiled\":\n",
    "        for _ in range(layers):\n",
    "            model.add(tf.keras.layers.Dense(units = unit,activation=func))\n",
    "        model.add(tf.keras.layers.Dense(output,activation=\"softmax\"))\n",
    "        model.compile(loss=lossfun, optimizer=optimizers, metrics= metric)\n",
    "    return model\n",
    "\n",
    "def lstmo(shape,state= \"compiled\",lstmlayer = 4,layers = 1,unitnormal = 100,func=\"relu\",output=11\n",
    ",unitlstm = 50,dropout= 0.5,lossfun = \"sparse_categorical_crossentropy\",optimizers = \"adam\",metric = [\"accuracy\"]):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.LSTM(units=unitlstm,return_sequences=True,input_shape=shape))\n",
    "    model.add(tf.keras.layers.Dropout(dropout))\n",
    "    for _ in range(lstmlayer-1):\n",
    "        model.add(tf.keras.layers.LSTM(units = unitlstm,return_sequences=True))\n",
    "        model.add(tf.keras.layers.Dropout(dropout))\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    if state == \"compiled\":\n",
    "        for _ in range(layers-1):\n",
    "            model.add(tf.keras.layers.Dense(units = unitnormal,activation=func))\n",
    "        model.add(tf.keras.layers.Dense(units = output,activation = \"softmax\"))\n",
    "        model.compile(loss=lossfun, optimizer=optimizers, metrics= metric)\n",
    "    return model\n",
    "\n",
    "#global identifier for metric measures to not be instantiated everytime function called\n",
    "verbo = {\"accuracy\":1,\"loss\":0,\"precision\":2,\"recall\":3}\n",
    "def fittertester(mod,train,train_target,test,test_target,batches = 32,epoch = 50,metric = \"accuracy\"):\n",
    "    \n",
    "    history = mod.fit(train,train_target,validation_split = 0.2\n",
    "        ,batch_size = batches,epochs = epoch,verbose = 1\n",
    "        ,callbacks = [\n",
    "            tf.keras.callbacks.EarlyStopping(\n",
    "                monitor = \"val_loss\",\n",
    "                patience = 5,\n",
    "                restore_best_weights=True\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    evaluation = mod.evaluate(test,test_target,verbose = 0)[verbo[metric]]\n",
    "    return history,evaluation\n",
    "\n",
    "#tihs works only on 1d cnn not even 2d neds to be optimized but idk how to rn\n",
    "def layervisualizer(model,layer,sample,path= \"./\",color = \"Spectral\"):\n",
    "    indices = []\n",
    "    for i,layers in enumerate(model.layers):\n",
    "        if layer in layers.name:\n",
    "            #get the index of the desired layer needed to visualize\n",
    "            indices.append(i)\n",
    "    #get_weights returns filters,biases so i get 0 because i want the filters weights only\n",
    "    filters = np.array([model.layers[ind].get_weights()[0] for ind in indices])\n",
    "    print(filters.shape[3])\n",
    "    for i in range(len(filters)):\n",
    "        for j in range(filters.shape[3]):\n",
    "            #this just means taking all the input values into consideration for each jth filter and ith layer\n",
    "            f = filters[i,:,:,j]\n",
    "            fig1 = plt.subplot(8,8,j+1)\n",
    "            fig1.set_xticks([])\n",
    "            fig1.set_yticks([])\n",
    "            plt.imshow(f,cmap = color,aspect = \"auto\")\n",
    "        plt.savefig(f\"{path}viz_{layer}_layer_{i+1}.png\",dpi = 1000)\n",
    "        plt.close()\n",
    "    \n",
    "    output = [model.layers[ind].output for ind in indices]\n",
    "    vizmodel = tf.keras.Model(inputs = model.inputs,outputs = output)\n",
    "    outp = vizmodel.predict(sample)\n",
    "    print(outp.shape)\n",
    "    #the for i in range is because the shape is (1,6270,64) and if i iterate\n",
    "    #over the outp itll only iterte for one time and wont iterate over each of the filters\n",
    "    for j in range(len(outp)):\n",
    "        for i in range(outp.shape[2]):\n",
    "            f = outp[:,:,i]\n",
    "            #subplot starts from 1\n",
    "            fig1 = plt.subplot(8,8,i+1)\n",
    "            fig1.set_xticks([])\n",
    "            fig1.set_yticks([])\n",
    "            plt.imshow(f,cmap = color,aspect = \"auto\")\n",
    "    plt.savefig(f\"{path}baseline_thru_{layer}_layer_{j+1}.png\",dpi = 1000)\n",
    "    plt.close()\n",
    "\n",
    "#abstracted into its own function because it only needs to be done once\n",
    "#and the multiheaded model could be initialized in the kfolds as its own module\n",
    "#if it doesnt work then probably each of the head models should be reinitialized everytime the multiheaded model is called\n",
    "#and if that happens the function built should be modified\n",
    "def flatter(models):\n",
    "    index = []\n",
    "    for model in models:\n",
    "        for i,layers in enumerate(model.layers):\n",
    "            if \"flatten\" in layers.name:\n",
    "                #get the index of the desired layer needed to visualize\n",
    "                index.append(i)\n",
    "    return index\n",
    "#im sure even if its not a problem it initializes the same random weights for each head\n",
    "#in each run so i dont know how much of a problem is it but it is only random on the heads\n",
    "#to remove that we have to reinitialize each model in each kfold as mentioned in the above section\n",
    "def headjoiner(models: list,index,layers = 1,unit = 100,func = \"softsign\",output = 11,lossfun = \"sparse_categorical_crossentropy\",optimizers = \"Adamax\",metric = [\"accuracy\"]):\n",
    "    #so why is it called by outputs makes it work for some reason i still dont know\n",
    "    #but it has something to do with teh sequential and functional api\n",
    "    #didnt work in this code\n",
    "    \"\"\"\n",
    "    produced by using this following code\n",
    "\n",
    "    def headjoiner(models: list,index,layers = 1,unit = 100,func = \"softsign\",output = 11,lossfun = \"sparse_categorical_crossentropy\",optimizers = \"Adamax\",metric = [\"accuracy\"]):\n",
    "    flats = [mod.layers[ind].output for ind,mod in zip(index,models)]\n",
    "    print(models[0].layers[index[0]]._inbound_nodes)\n",
    "    flats = np.array(flats)\n",
    "    ###########################################################\n",
    "    #this istn considered as aviable input needs to be checked\n",
    "    merged = tf.keras.layers.concatenate(inputs = flats,axis =-1)\n",
    "    #couldnt use it as add since merged isnt considered as a model you could add to \n",
    "    for _ in range(layers):\n",
    "        merged = tf.keras.layers.Dense(units = unit,activation=func)(merged)\n",
    "    merged = tf.keras.layers.Dense(output,activation=\"softmax\")(merged)\n",
    "    inputs = [model.inputs for model in models]\n",
    "    mergedmod = tf.keras.Model(input = inputs,output = merged)\n",
    "    mergedmod.compile(loss=lossfun, optimizer=optimizers, metrics= metric)\n",
    "    return mergedmod\n",
    "\n",
    "    TypeError: You are passing KerasTensor(type_spec=TensorSpec(shape=(None, 80256), \n",
    "    dtype=tf.float32, name=None), name='flatten_16/Reshape:0', description=\"created by layer 'flatten_16'\"),\n",
    "    an intermediate Keras symbolic input/output, to a TF API that does not allow registering custom dispatchers,\n",
    "    such as `tf.cond`, `tf.function`, gradient tapes, or `tf.map_fn`. Keras Functional model construction\n",
    "    only supports TF API calls that *do* support dispatching, such as `tf.math.add` or \n",
    "    `tf.reshape`. Other APIs cannot be called directly on symbolic Kerasinputs/outputs. \n",
    "    You can work around this limitation by putting the operation in a custom \n",
    "    Keras layer `call` and calling that layer on this symbolic input/output\n",
    "    \n",
    "    #might be helpful\n",
    "    https://stackoverflow.com/questions/44042173/concatenate-merge-layer-keras-with-tensorflow\n",
    "    \n",
    "    https://keras.io/guides/functional_api/\n",
    "\n",
    "    https://www.google.com/search?q=How+to+%22Merge%22+Sequential+models+in+tensorflow&ei=VkrfY97XGqvUkdUP0IiOkAY&ved=0ahUKEwje2rWg3v38AhUraqQEHVCEA2IQ4dUDCA8&uact=5&oq=How+to+%22Merge%22+Sequential+models+in+tensorflow&gs_lcp=Cgxnd3Mtd2l6LXNlcnAQAzIFCCEQoAE6CAgAEIYDELADSgQIQRgASgQIRhgAUPUEWLMGYJwIaABwAHgAgAGXAYgBhwOSAQMwLjOYAQCgAQHIAQXAAQE&sclient=gws-wiz-serp\n",
    "    \n",
    "    really useful link as well:\n",
    "    https://stackoverflow.com/questions/53942291/what-does-the-00-of-the-layers-connected-to-in-keras-model-summary-mean\n",
    "\n",
    "    explains how each layer is a node and carries the structure along wiht it no need to \n",
    "    \"\"\"\n",
    "    #note on .output\n",
    "    flats = [mod.layers[ind].output for ind,mod in zip(index,models)]\n",
    "    #print(models[0].layers[index[0]]._inbound_nodes)\n",
    "    ###########################################################\n",
    "    #this istn considered as aviable input needs to be checked\n",
    "    merged = tf.keras.layers.Concatenate()(flats)\n",
    "    #couldnt use it as add since merged isnt considered as a model you could add to \n",
    "    for _ in range(layers):\n",
    "        merged = tf.keras.layers.Dense(units = unit,activation=func)(merged)\n",
    "        merged = tf.keras.layers.Dropout(0.5)(merged)\n",
    "    merged = tf.keras.layers.Dense(output,activation=\"softmax\")(merged)\n",
    "    inputs = [model.inputs for model in models]\n",
    "    \n",
    "    mergedmod = tf.keras.Model(inputs,merged)\n",
    "    mergedmod.compile(loss=lossfun, optimizer=optimizers, metrics= metric)\n",
    "    return mergedmod\n",
    "\n",
    "def resultviz(path,model,hist,actual,test,uniques):\n",
    "    #predictions is the max probability over the predictions since each prediction element is an array of length 11 with probvabliities of each class\n",
    "    prediction = np.array(list(map(lambda x:np.argmax(x),model.predict(test))))\n",
    "    cs = confusion_matrix(actual,prediction)\n",
    "    clr = classification_report(actual,prediction,target_names = uniques,output_dict=True)\n",
    "    plt.figure(figsize = (10,10))\n",
    "    sns.heatmap(cs,annot = True,cmap = \"GnBu\")\n",
    "    plt.xticks(np.arange(len(uniques)),remapper)\n",
    "    plt.yticks(np.arange(len(uniques)),remapper)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.savefig(f\"{path}Heatmap.png\")\n",
    "    print(model.summary())\n",
    "    plt.close()\n",
    "    plt.plot(hist.history[\"loss\"],label=\"train loss\")\n",
    "    plt.plot(hist.history[\"val_loss\"],label=\"validation loss\")\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"{path}Trainingloss.png\")\n",
    "    plt.close()\n",
    "    plt.plot(hist.history['accuracy'],label='accuracy')\n",
    "    plt.savefig(f\"{path}accuracy.png\")\n",
    "    print(\"here\")\n",
    "    dataframe = pd.DataFrame(clr)\n",
    "    dataframe.to_csv(f\"{path}{model.layers[0].name}_report.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data = pd.read_pickle(\"E:/ABDO/Graduation project/Datasets/Emognition/hr(tuples).pkl\")\n",
    "\n",
    "    #it is turned into an object i dont know why but it's turned into an inta nd type changed before\n",
    "    #mask so i dont pass it through another func\n",
    "    remapper = data[\"Target\"].unique()\n",
    "    data[\"Target\"] = encoder(data,\"Target\")\n",
    "    #no need since new data is formed in hr only\n",
    "    data = splittuple(data,0)\n",
    "    databvp = pd.read_pickle(\"E:/ABDO/Courses/Python/GraduationProject/Emognition/bvp_clean.pkl\")\n",
    "    data = data[data[\"Stage\"] == \"STIMULUS\"]\n",
    "    databvp = databvp[databvp[\"Stage\"] == \"STIMULUS\"]\n",
    "    #databvp = splittuple(databvp,1)\n",
    "    # get the target values\n",
    "    #put in an np.array as int idk why but it used to work without it now it \n",
    "    #reads its type not as an int but as an object while every var inside is an instance of an int\n",
    "\n",
    "#data splitting, and model prepping\n",
    "########################################################################################################################################################\n",
    "    datatarget = np.asarray(data[\"Target\"].values).astype(int)\n",
    "    #adding pp interval is dstacking two splits one split for the hr and one for the ppinterval\n",
    "    databvp = np.asarray(databvp.values[:,3:]).astype(float)\n",
    "    data =  np.asarray(data.values[:,2:-13]).astype(int)\n",
    "\n",
    "    model1 = cnn1d((data.shape[1],1))\n",
    "    model2 = cnn1d((databvp.shape[1],1))\n",
    "    #just for ease in writing\n",
    "    modlist = [model1,model2]\n",
    "    index = flatter(modlist)\n",
    "    \n",
    "#model training, and visualization\n",
    "###########################################################################################################################################################   \n",
    "    kfolds = StratifiedKFold(n_splits=4)\n",
    "    models = []\n",
    "    accuracy = []\n",
    "    histories = []\n",
    "    tests = [] "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "data = pd.read_pickle(\"E:/ABDO/Courses/Python/GraduationProject/Emognition/final.pkl\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "data[\"Target\"] = encoder(data,\"Target\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "model1 = cnn1d((data.shape[1],1))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "data.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(5669, 100)"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "data = np.asarray(data.values[:,2:102]).astype(int)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "kfolds = StratifiedKFold(n_splits = 2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "datatarget = np.asarray(data[\"Target\"].values).astype(int)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "models = []\n",
    "accuracy = []\n",
    "histories = []\n",
    "tests = []\n",
    "targets = []\n",
    "for train,test in kfolds.split(data,datatarget):\n",
    "    X_train,X_test = data[train],data[test]\n",
    "    train_target,test_target = datatarget[train],datatarget[test]\n",
    "    mod = cnn1d((data.shape[1],1))\n",
    "    history,evalu = fittertester(mod,X_train,train_target,X_test,test_target)\n",
    "    accuracy.append(evalu)\n",
    "    histories.append(history)\n",
    "    tests.append(X_test)\n",
    "    models.append(mod)\n",
    "    targets.append(test_target)\n",
    "print(f\"mean accuracy{np.mean(accuracy)} and standard deviation:{np.std(accuracy)}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/50\n",
      "71/71 [==============================] - 3s 8ms/step - loss: 2.3876 - accuracy: 0.1085 - val_loss: 2.3890 - val_accuracy: 0.1093\n",
      "Epoch 2/50\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 2.3760 - accuracy: 0.1032 - val_loss: 2.4194 - val_accuracy: 0.1358\n",
      "Epoch 3/50\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 2.3722 - accuracy: 0.1094 - val_loss: 2.4098 - val_accuracy: 0.1376\n",
      "Epoch 4/50\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 2.3721 - accuracy: 0.1156 - val_loss: 2.4167 - val_accuracy: 0.1376\n",
      "Epoch 5/50\n",
      "71/71 [==============================] - 0s 6ms/step - loss: 2.3656 - accuracy: 0.1067 - val_loss: 2.4268 - val_accuracy: 0.1093\n",
      "Epoch 6/50\n",
      "71/71 [==============================] - 0s 6ms/step - loss: 2.3690 - accuracy: 0.1217 - val_loss: 2.4532 - val_accuracy: 0.0406\n",
      "Epoch 1/50\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 2.3548 - accuracy: 0.1393 - val_loss: 2.8088 - val_accuracy: 0.0794\n",
      "Epoch 2/50\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 2.3374 - accuracy: 0.1424 - val_loss: 2.8289 - val_accuracy: 0.0794\n",
      "Epoch 3/50\n",
      "71/71 [==============================] - 0s 4ms/step - loss: 2.3326 - accuracy: 0.1486 - val_loss: 2.9294 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/50\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 2.3313 - accuracy: 0.1481 - val_loss: 2.8102 - val_accuracy: 0.0018\n",
      "Epoch 5/50\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 2.3317 - accuracy: 0.1393 - val_loss: 2.9484 - val_accuracy: 0.0794\n",
      "Epoch 6/50\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 2.3324 - accuracy: 0.1481 - val_loss: 2.9463 - val_accuracy: 0.0018\n",
      "mean accuracy0.12577295675873756 and standard deviation:0.006901707500219345\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "models = []\n",
    "accuracy = []\n",
    "histories = []\n",
    "tests = []\n",
    "targets = []\n",
    "for train,test in kfolds.split(data,datatarget):\n",
    "    X_train,X_test = data[train],data[test]\n",
    "    train_target,test_target = datatarget[train],datatarget[test]\n",
    "    mod = lstmo((data.shape[1],1))\n",
    "    history,evalu = fittertester(mod,X_train,train_target,X_test,test_target)\n",
    "    accuracy.append(evalu)\n",
    "    histories.append(history)\n",
    "    tests.append(X_test)\n",
    "    models.append(mod)\n",
    "    targets.append(test_target)\n",
    "print(f\"mean accuracy{np.mean(accuracy)} and standard deviation:{np.std(accuracy)}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/50\n",
      "71/71 [==============================] - 8s 41ms/step - loss: 2.3766 - accuracy: 0.1107 - val_loss: 2.4356 - val_accuracy: 0.0406\n",
      "Epoch 2/50\n",
      "71/71 [==============================] - 2s 23ms/step - loss: 2.3265 - accuracy: 0.1270 - val_loss: 2.4613 - val_accuracy: 0.1129\n",
      "Epoch 3/50\n",
      "71/71 [==============================] - 2s 24ms/step - loss: 2.3176 - accuracy: 0.1367 - val_loss: 2.3924 - val_accuracy: 0.0829\n",
      "Epoch 4/50\n",
      "71/71 [==============================] - 2s 26ms/step - loss: 2.3227 - accuracy: 0.1200 - val_loss: 2.4477 - val_accuracy: 0.0794\n",
      "Epoch 5/50\n",
      "71/71 [==============================] - 2s 26ms/step - loss: 2.3112 - accuracy: 0.1442 - val_loss: 2.4247 - val_accuracy: 0.0882\n",
      "Epoch 6/50\n",
      "71/71 [==============================] - 2s 24ms/step - loss: 2.2932 - accuracy: 0.1429 - val_loss: 2.6880 - val_accuracy: 0.1464\n",
      "Epoch 7/50\n",
      "71/71 [==============================] - 2s 24ms/step - loss: 2.2775 - accuracy: 0.1592 - val_loss: 2.6550 - val_accuracy: 0.1217\n",
      "Epoch 8/50\n",
      "71/71 [==============================] - 2s 24ms/step - loss: 2.2485 - accuracy: 0.1566 - val_loss: 2.8392 - val_accuracy: 0.1252\n",
      "Epoch 1/50\n",
      "71/71 [==============================] - 7s 44ms/step - loss: 2.3362 - accuracy: 0.1420 - val_loss: 2.5326 - val_accuracy: 0.1640\n",
      "Epoch 2/50\n",
      "71/71 [==============================] - 2s 24ms/step - loss: 2.3058 - accuracy: 0.1407 - val_loss: 2.7068 - val_accuracy: 0.0776\n",
      "Epoch 3/50\n",
      "71/71 [==============================] - 2s 24ms/step - loss: 2.3023 - accuracy: 0.1512 - val_loss: 2.6203 - val_accuracy: 0.0917\n",
      "Epoch 4/50\n",
      "71/71 [==============================] - 2s 25ms/step - loss: 2.2767 - accuracy: 0.1645 - val_loss: 2.5914 - val_accuracy: 0.1587\n",
      "Epoch 5/50\n",
      "71/71 [==============================] - 2s 24ms/step - loss: 2.2608 - accuracy: 0.1574 - val_loss: 2.4885 - val_accuracy: 0.1658\n",
      "Epoch 6/50\n",
      "71/71 [==============================] - 2s 24ms/step - loss: 2.2365 - accuracy: 0.1693 - val_loss: 2.4432 - val_accuracy: 0.1658\n",
      "Epoch 7/50\n",
      "71/71 [==============================] - 2s 25ms/step - loss: 2.2189 - accuracy: 0.1750 - val_loss: 2.5441 - val_accuracy: 0.1182\n",
      "Epoch 8/50\n",
      "71/71 [==============================] - 2s 26ms/step - loss: 2.1787 - accuracy: 0.1918 - val_loss: 2.4034 - val_accuracy: 0.2063\n",
      "Epoch 9/50\n",
      "71/71 [==============================] - 2s 23ms/step - loss: 2.1690 - accuracy: 0.1940 - val_loss: 2.4927 - val_accuracy: 0.1041\n",
      "Epoch 10/50\n",
      "71/71 [==============================] - 2s 22ms/step - loss: 2.1559 - accuracy: 0.2138 - val_loss: 2.5901 - val_accuracy: 0.1182\n",
      "Epoch 11/50\n",
      "71/71 [==============================] - 2s 22ms/step - loss: 2.1298 - accuracy: 0.2086 - val_loss: 2.7303 - val_accuracy: 0.0617\n",
      "Epoch 12/50\n",
      "71/71 [==============================] - 2s 22ms/step - loss: 2.1595 - accuracy: 0.1997 - val_loss: 2.5194 - val_accuracy: 0.1252\n",
      "Epoch 13/50\n",
      "71/71 [==============================] - 2s 22ms/step - loss: 2.1136 - accuracy: 0.2213 - val_loss: 2.5197 - val_accuracy: 0.1446\n",
      "mean accuracy0.10619356483221054 and standard deviation:0.011308200657367706\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 4
 }
}